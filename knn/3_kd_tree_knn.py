# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 2/22/19 10:47 AM
 @desc: 针对KD Tree算法背后原理的理解
"""

"""
 1. KD Tree算法介绍: KD树就是K个特征维度的树，注意这里的K和KNN中的K的意思不同。KNN中的K代表最近的K个样本，KD树中的K代表样本特征的维数
 2. KD Tree算法步骤: 1.建立KD树  2.搜索最近邻  3.KD树预测
 3. 如何建立KD Tree:
    (1) 从m个样本的n维特征中,分别计算n个特征的方差,选取方差值最大的那一维特征,计算其中位数,中位数所在的那一个样本点,作为根节点;
    (2) 将其余样本点划分为左右子树,左子树是取值小于父节点的分类特征取值的所有样本,右子树反之;
    (3) 左右字树重复(1)操作，直至所有数据都被建立到KD Tree的节点上;
 4. 如何搜索目标点的最近邻点:
    (1) 从树的根节点开始,比较当前节点的分割特征值与目标节点的对应值的大小,决定向左或者向右移动;
    (2) 待测节点向下搜索,直至到达KD Tree的叶子节点,将该叶子节点保存为“当前最近邻节点”;
    (3) 开始回溯过程，即从叶子节点返回到树的根节点,回溯过程如下:
        a. 如果当前节点比“当前最近邻节点”距目标节点更近的话,则保存该节点为“当前最近邻节点”；
        b. 检查另一个子节点对应的空间是否与 以目标点为球心、目标点与“当前最邻近点”的距离为半径的超球体相交;
            如果相交的话,可能在另一个子节点对应的区域，存在点距离目标点更近,于是回溯到另一个子节点去,重新计算最邻近点;
            如果不相交的话,直接向上回退;
    (4) 当最后一次回退到根节点时,搜索结束,最后的"当前最邻近点"即为目标点的最邻近点
    推荐结合《统计学习方法》P44 例3.3 来了解具体搜索过程
 5. KD树预测:
    (1) 在上述KD树搜索最近邻的基础上,我们获取到了第一个最近邻的样本点，并把它标注为已选;
    (2) 开始第二轮KD树最近邻搜索过程，忽略“已选”的节点,获得第二个最近邻的样本点,继续标注为已选;
    (3) 重复k轮,这样就得到了目标节点的K个最近邻;
    (4) 分类任务就采用用多数表决法,回归任务就用平均数或其他方式;
 6. KD树的时间复杂度是处于O(logN)与O(N),这里的N是训练的样本数,KD树更适用于训练样本数远大于样本维度时的情况;
    当样本数与维度差不多时，kd-tree搜索与暴力搜索(线性扫描)效率没什么区别
 【这里只是了解了kd-tree的算法原理,没有手动实现kd-tree的代码】
 
 【补充】本文的构建KD树算法思路与李航书中不同的原因：
    李航书中的knn是最早出现的KNN算法的原理，kd树建立就是特征轮流来。后来对kd树的一个重大改进就是特征的选取，用方差选择后建立的kd树搜索近邻效率更高。
因此现在主流knn类库中kd树建立都不再是轮流而是基于方差选择了了。
    如果最大特征方差有相同的话就随机选择一个即可。
"""
