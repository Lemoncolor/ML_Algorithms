# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/4/19 3:36 PM
 @desc: 以函数f(x) = x^2 - 4x 为例，学习如何用梯度下降找到极小值
"""

"""
 思路：
    1.按照高中数学的思路，首先是对函数求导，导数为零的点，就是极值，一眼看出极值点 x=2
    2.遇到复杂的函数，恐怕就没有这么简单，我们改用迭代的方法，就像下坡一样，一步一步地逼近极值
    3.如何用数学语言描述“一步一步”呢？这里就想到了函数的瞬时变化率，即导数，涉及到多变量时，则用偏导数，表示在某一个变量上的变化率
        具体说一下吧：Δy = f(x_0+Δx)-f(x_0),当Δx->0的时候，Δy/Δx即为瞬时变化率，或者说是在x=x_0处的导数
                         “一步一步”  :   x_0' = x_0 - α*Δy/Δx  , 其中 α是一个大于0的正数，取值比较小，如0.001
        如果x_0在函数f(x)= x^2 - 4x 的极小值的左边，那么在x_0处的导数小于0，即Δy/Δx<0，x_0'的值增大，即x_0向极小值移动；
        如果x_0在函数f(x)= x^2 - 4x 的极小值的右边，那么在x_0处的导数大于0，即Δy/Δx>0，x_0'的值减小，即x_0向极小值移动；
        
    4.于是下坡的动作，用数学公式表达为 x_i+1 = x_i - α * f'(x_i)/x_i   # 当涉及到多变量时，此处应该用偏导，这里为了方便，写的导数符号
    5.我们知道如何下坡了，那怎么表达我们已经到山底了呢？经过前面的讲解，我们知道x在不断地向极小值逼近，y的变化量也是越来越小，我们不妨定义
      一个收敛因子ε(如ε=0.000001)，当Δy<ε时，我们认为找到了函数的极小值.
    
 按照上面的思路，我们开始编写代码
"""


# 定义默认的函数f(x)
def f(x):
    return x ** 2 - 4 * x


# 返回f(x) = x^2-4x 在x处的导数值
def f_prime(x):
    return 2 * x - 4


if __name__ == "__main__":
    convergence = 0.000001
    x_old = 0  # 初始化x为0
    alpha = 0.001
    delta_y = 1  # 随便初始一个大于convergence的值
    while abs(delta_y) > convergence:
        x_new = x_old - alpha * f_prime(x_old)
        delta_y = f(x_new) - f(x_old)
        x_old = x_new
    print("gradient descent search result x is ", x_old)
    print("the function's minimum value is ", f(x_old))

"""
 函数输出为:
     gradient descent search result x is  1.9842308105854123
     the function's minimum value is  -3.9997513326652068
 
 我们知道，真实的极小值点为2，此时函数的取值为-4，发现与梯度下降计算的结果相差不大，继续缩小convergence，结果也会越来越接近.
 我发现把alpha设为0.01时，梯度下降结果反而更接近真实值，alpha设为0.0001时，梯度下降结果还没有alpha=0.001时好，这说明我们的学习步长
 还是要和convergence对应起来，并不是越小越好； 
 或者另一个办法，设置学习步长和迭代轮数，步长越小，迭代轮数越大，最后结果也会越来越接近真实值。
    
 补充:
    梯度的概念，梯度是一个向量，里面的每一个元素是函数对某个自变量的偏导数，梯度是有方向的，函数在该点处沿着该方向变化最快。
    梯度上升:自变量x 加上 该点处的梯度值*步长，向函数值增大的方向移动
    梯度下降:自变量x 减去 该点处的梯度值*步长，向函数值减小的方向移动
"""
