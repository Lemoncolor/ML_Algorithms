# encoding: μtf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 4/1/19 8:56 AM
 @desc: 从原理上对K-Means算法进行比较详细的介绍，下一个小结以一个简单的demo来实现k-means代码
"""

"""
 1.K-Means原理
 
    (1)k-means算法的思想:
        
        对于给定的样本集，按照样本之间的距离大小，将样本划分为K个簇，让簇内的样布点尽量紧密连在一起，而让簇间的距离尽量的大.
        
    (2)数据表达式显示，假设簇的划分为(C1,C2,C3,...,Ck)，则我们的目标是最小化平方误差E:
        
        E = sμm (xi-μ1)^2 + sμm (xj-μ2)^2 + ... + sμm (xm-μk)^2     其中xi∈C1，xj∈C2，...，xm∈Ck，μ1,μ2...分别是每个簇的均值
        
 2.传统的K-Means算法流程
 
    输入是样本集D = {x1, x2, x3, ..., xm}，聚类的簇数k，最大迭代次数N
    
    输出是簇划分C = {C1, C2, C3, ... , Ck}
    
    (1)从数据集D中随机选择k个样本作为初始的k个质心向量：{μ1,μ2,μ3,...,μk}
    
    (2)对于n = 1, 2, 3, ..., N（max_iter）
        
        (a)对于i=1,2,3,...,M， 计算样本点xi与每一个质心向量μj的距离dij = (xi-μj)^2，将xi划分给距离最小的dij所对应的类别
        
        (b)所有样本点划分完毕之后，重新计算每个簇的质心向量，用均值来计算
        
        (c)如果所有质心都没发生变化，则转至步骤(3)
        
    (3)输出簇划分C = {C1, C2, C3, ... , Ck}
    
 3.K-Means的初始化优化算法K-Means++
 
    K-Means算法初始时，k个质心的位置的选择对最后聚类的结果和运行时间影响很大，因此需要选择合适的k个质心，如果仅仅是随机的选择，有可能导致算法
    收敛很慢，于是提出K-Means++算法，对初始化质心的方法进行优化.
 
    (1)从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1
    
    (2)对于数据集中的每一个点xi，计算它与现有的所有聚类中心的距离 D(xi) = agr min (xi-μr)^2 , r = 1,2,3,...,k_selected
    
    (3)选择一个新的数据点作为聚类中心，选择的原则是D(x)中较大的点被选中为聚类中心的概率较大
    
    (4)重复(2)和(3)过程，直至选择出k个聚类中心
    
    (5)利用这k个聚类中心来执行K-Means算法
    
4.K-Means距离计算优化elkan K-Means

    在传统的K-Means算法中，我们在每轮迭代时，要计算所有的样本点到所有的质心的距离，这样会比较耗时。那么，对于距离的计算有没有能简化的地方呢？
    elkan K-Means算法就是从这块入手加以改进。它的目标是减少不必要的距离的计算。那么哪些距离不需要计算呢？

　　 elkan K-Means利用了两边之和大于等于第三边,以及两边之差小于第三边的三角形性质，来减少距离的计算.
    
    (1)对于一个样本点x和两个质心μ1，μ2，如果我们预先算出这两个质心之间的距离D(μ1,μ2)，如果发现2*D(x,μ1)<=D(μ1,μ2)，而又根据两边之和大于
       第三边知D(x,μ1) + D(x,μ2) > D(μ1,μ2)，可直接得出D(x,μ1) < D(x,μ2)，当x在两个质心连线的中点处时，可以取等号；
       
    (2)对于一个样本点x和两个质心μ1，μ2，我们可以得到D(x,μ2) >= max {0,D(x,μ1)-D(μ1,μ2)} （两边之差小于第三边）
 
5.大样本优化Mini Batch K-Means
    传统的K-Means算法中，要计算所有的样本点到所有的质心的距离。如果样本量非常大，比如达到10万以上，特征有100以上，此时传统的K-Means算法非常
 的耗时，于是Mini Batch算法应运而生。
 
    顾名思义，就是用样本集中的一部分的样本来做传统的K-Means，这样降低了计算量，此时的代价就是聚类的精确度会有一些降低，在Mini Batch K-Means
 中，我们会选择一个合适的batch size，我们仅仅用batch size个样本来做K-Means聚类。那么这batch size个样本怎么来的？一般是通过无放回的随机采样
 得到的.为了增加算法的准确性，我们一般会多跑几次Mini Batch K-Means算法，用得到不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。
 
6.K-Means与KNN
    
    经常容易把K-Means和KNN搞混，两者其实差别挺大的
    
    K-means是无监督学习的聚类算法，没有样本输出；
    
    KNN是有监督学习的分类算法，有对应的类别输出，KNN基本不需要训练，对于测试集里面的点，只需要找到样本集中最近的k个点，用这个k个点的类别来预测
 测试点的类别，而K-Means则需要进行训练，找到k个类别的聚类中心，从而决定簇的类别.
 
    二者也有一些相似点，比如说都要找和某一个点最近的点，两者都用到了最近邻(nearest neighbors)的思想.
    
7.K-Means小结
    
    K-Means是一个简单实用的聚类算法，这里对K-Means的优缺点做一个总结
    
    优点：
        (1) 原理比较简单，实现起来比较容易
        (2) 聚类效果较优
        (3) 算法的可解释性较强
        (4) 主要需要调的参数少，仅仅是簇数k
        
    缺点：
        (1) 在高维度特征空间时，k的选择不太好把握
        (2) 如果各隐含类别的数据不平衡，则聚类效果不佳
        (3) 采用迭代方法，得到的结果只能是局部最优
        (4) 对噪声点和异常点比较敏感
        
"""
