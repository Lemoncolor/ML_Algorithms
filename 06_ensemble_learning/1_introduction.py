# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/22/19 9:31 AM
 @desc:本文不涉及代码，主要是针对集成学习进行一个全面的介绍
"""
"""
 引子：
     在机器学习的监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好
  的模型（弱监督模型，在某些方面表现的比较好）.集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便
  某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来.

 1.集成学习的类别
    
     集成学习有三种常见的学习框架，分别是bagging、boosting和stacking，下面针对这三种框架分别进行介绍。
    
    (1)Bagging
        Bagging是bootstrap aggregating的简写。先说一下bootstrap，也称自助法，它是一种有放回的抽样方法，一般用于数据集比较小的情况。在
    Bagging方法中，首先从整体数据集中进行有放回地抽样，得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果由这N个模型的输出所决定，
    具体地：
        分类问题采用N个模型多数投票的原则来决定；
        回归问题采用N个模型取平均值的方法来决定；
        
        RandomForest就属于Bagging方式，或者说RandomForest是Bagging方法的一个特化进阶版，所谓特化，是因为RandomForest算法中，弱学习器
    已经指定了全部为决策树模型，所谓进阶，是因为RandomForest算法中，在bagging对样本随机采样的基础上，又加上了在抽样时，对每个数据集中样本的
    特征，也是随机选择。
    
        直白的说，随机森林算法在对数据集进行有放回采样时，每次不仅会对数据集的行（样本数）随机采样，而且还会对数据集的列（特征数）进行随机采样.
    我们在前面学过决策树算法就知道，这样子做的好处就是让每一棵子树不容易发生过拟合.
    
        预测的时候，随机森林中的每一棵树都对输入进行预测，分类问题就通过多数投票方式返回预测值，回归问题就通过取平均值方式返回预测值.
        
    (2)Boosting
        Boosting(提升)方法是一种可以用来减小监督学习中偏差的机器学习算法，它通过迭代的方式，训练一系列的分类器，每个分类器采用的样本的选择
    方式，都和上一轮的学习结果有关.
    
        Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1
    学习误差率高的训练样本点的权重变高，从而这些误差率高的样本点在后面的弱学习器2中会得到更多的重视。之后基于权重调整后的训练集来训练弱学习器2.
    如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器.
    
        Boosting方式中，具有代表性的有AdaBoost(Adaptive Boosting)算法和Boosting Tree(提升树)算法，另外在提升树算法中，应用最广泛的是
    GBDT(gradient boosting decision tree)梯度提升树算法，之后会详细介绍这两种算法.
    
        预测的时候，boosting不同于bagging. 在bagging算法中，每一个base model的权重是一样的，而在boosting算法中,每个base model的权重
    是不一样的,每一个权重代表的是其对应分类器在上一轮迭代中的成功度，因此对于分类问题就采用加权投票法，每个弱分类器的分类票数要乘以一个权重，最
    终将各个类别的加权票数求和，最大的值对应的类别为最终类别，回归问题也同理，每个模型预测值的加权和作为模型输出.
    
    
    (3)Stacking
        Stacking方法，简单来说，就是训练一个多层(一般两层就够了)的学习器结构，第一层(也叫学习层)用n个不同的base model，对样本集进行训练，
    然后每一个基模型再对样本进行预测，每一列预测结果组成了一列新的特征，最终所有预测结果组成的数据集，作为新的特征集，输入给下一层的模型。
    
        给出一个比较简单的例子：你想要预测明天是否会下雨，并有相关的气象数据。你训练了十个分类器比如LR，svm，knn等。你通过stacking的方法在
    第一层将这十个分类器的结果作为了第二层分类器的特征集，通过第二层的输出，得到了最终预测结果.
    
        另外，为了降低过拟合的问题，第二层分类器应该是较为简单的分类器，广义线性如逻辑回归是一个不错的选择。在特征提取的过程中，我们已经使用了
    复杂的非线性变换，因此在输出层不需要复杂的分类器。
    
        Stacking和神经网络从某种角度看有异曲同工之妙，神经网络也可以被看作是集成学习。Stacking中的第一层可以等价于神经网络中的前 n-1层，而
    Stacking中的最终分类层可以类比于神经网络中最后的输出层，区别在于：
            - Stacking需要宽度，而深度学习不需要     (对Stacking第一层的base model要求两点：差异性要大 和 准确度要高)
            - 深度学习需要深度，而Stacking不需要
 
 2.偏差和方差
    广义的偏差(bias)，描述的是预测值和真实值之间的差异，广义的方差(variance)描述的是预测值本身作为随机变量的离散程度，与真实值无关.参看目录
 下的bias_variance.png文件，这张图是让我们在广义上对偏差和方差的概念有一定的理解，下面主要是介绍模型的方差与偏差，注意区分。
    
    (1)模型的偏差和方差是什么?
        模型的偏差是一个比较简单的概念：即训练出来的模型在训练集上的准确度
        
        要解释模型的方差，首先要重新审视模型 ： 那就是要把模型理解为随机变量!!!  千万不要认为模型的方差就是它预测值的方差.
        
        先来看一下目录中的bias_variance_2.jpg文件，图片中是四个不同的模型在同一个数据集上的训练结果，不难得出以下两个结论：
            (a)左上方的模型，偏差是最大的，右下角的模型，偏差是最小的；
            (b)左上方的模型，方差是最小的，右下角的模型，偏差是最大的；
            
            第一个结论好理解，一眼都能看出来，如何理解第二个结论呢?
            不妨看一下目录中的bias_variance_3.jpg文件，蓝色和绿色分别是同一个训练集上采样得到的两个训练子集，由于采取了复杂的算法去拟合，
        所以最终得到的两个模型，显然二者的差异很大；但是如果采取直线来拟合的话，最后得到的两个模型就不会有那么大的差异了.
            
       结论： 一般来说，模型越复杂，越容易出现过拟合，导致模型的偏差越来越小，而模型的方差却越来越大，因此在算法学习中，想要模型达到最优的预测
    效果，一定要同时兼顾bias与variance，取二者的值都相对较低时，对应的模型!   (见bias_variance_4.jpg 这张图一定要好好理解)

    (2)集成学习是如何权衡偏差与方差的呢?
        我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型.但是，并不是所有
    集成学习框架中的基模型都是弱模型。Bagging和Stacking中的基模型为强模型（偏差低方差高），Boosting中的基模型为弱模型.
    
        (a)Bagging
            由于子样本集的相似性，并且使用的是同种模型，所以对于bagging模型， 有：
                - 整体模型的偏差近似于base model的偏差；     (这样也是bagging采用强学习器的原因)
                - 整体模型的方差小于等于base model的方差；   (base model相关性为1时取等)
            用公式可以一目了然：
                设有n个随机变量，方差为σ^2，两两变量之间的相关性为ρ，则方差为： ρ * σ^2 + (1-ρ) * σ^2 / n
                
                随着基模型的个数n的增多，整体模型的方差减少，防过拟合能力增强，模型的准确度上升；当基模型增大到一定程度后，对方差也不会再带来
            太大的改变了，这便是准确度的极限.
        
                bagging方法降低后面的第二项,来减少方差,而随机森林呢?则是同时降低了两项,也就是说还降低了base model之间的相关性. 众所周知
            bagging的base model之间的相关性是较高的.因为采用的是同一个模型,训练子集是有放回采样得到的,所以base model之间不是相互独立的,
            而随机森林在采样的时候,同时还对特征数也进行了随机采样,就是说增大了不同训练子集之间的差异性,从而训练得到的base model之间的相关性
            也就降低了。因此随机森林的variance更小，模型最终预测的准确度更高.
            
        (b)Boosting
            对于boosting而言，我们知道它的每一个base model之间是强相关的，那么模型的相关系数近似等于1
            我们同样用公式表示：
                                方差为： n^2 * γ^2 * σ^2
            
            可以发现，如果基模型是强学习器(复杂模型)，那么方差就会较大，导致整体模型的方差也就较大，整体模型发生过拟合，因此boosting框架中，
        一定要用弱学习器作为base model.
        
            因为基模型为弱模型，导致了每个基模型的准确度都不是很高。随着基模型数的增多，整体模型的期望值增加，更接近真实值，因此，整体模型的
        准确度提高。但是准确度一定会无限逼近于1吗？仍然并不一定，因为训练过程中准确度的提高的主要功臣是整体模型在训练集上的偏差减小，从而准确
        度提高，而随着训练的进行，基模型数越多，整体模型的方差变大，导致防过拟合的能力变弱，最终导致了准确度反而有所下降.
        
    (3)小结
        集成学习模型的调参工作的核心就是找到合适的参数，能够使整体模型在训练集上的准确度(偏差)和防止过拟合(方差)的能力达到协调，从而达到在样本
    总体上的最佳准确度!
    
        而对于集成学习中的要调的参数，现在可以粗略地分为两类了：
            (a)控制整体模型训练过程的参数
            (b)基模型的参数
        这两类参数都在影响着模型在训练集上的准确度以及防止过拟合的能力
"""
