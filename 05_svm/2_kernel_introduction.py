# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/12/19 5:13 PM
 @desc: 不涉及代码，简单聊一聊kernel trick
"""

"""
 1.回顾多项式回归
    比如一个只有两个特征的p次方多项式回归的模型：
            h_θ(x1,x2) = θ0 + θ1 * x1 + θ2 * x2 + θ3 * x1^2 + θ4 * x2^2 + θ5 * x1 * x2
   
    如果令 x0=1,x1=x1,x2=x2,x3=x1^2,x4=x2^2,x5=x1*x2,这样我们就得到了下式：
            h_θ(x1,x2) = θ0 + θ1 * x1 + θ2 * x2 + θ3 * x3 + θ4 * x4 + θ5 * x5
            
    可以发现，我们又重新回到了线性回归，这是一个五元线性回归，可以用线性回归的方法来完成算法，对于每一个二元样本特征(x1,x2)，我们得到了一个五元
 样本特征(x1,x2,x1^2,x2^2,x1x2)，通过这个改进的五元样本特征，我们重新把不是线性回归的函数变回线性回归。
 
    也就是说，对于二维不是线性的数据，我们将其映射到了五维之后，就变成线性的数据了。这给了我们启发，对于SVM线性不可分的低纬度数据，我们同样可以
 映射到高维，就能线性可分。
 
 2.核函数的引入
    先回顾一下线性可分的SVM的优化目标函数(见同目录下的 opt_object_function.png)
                            min 1/2 * ( sum(αi*αj*yi*yj*(xi∙xj)) - sum(αi) ) ,其中i,j = 1,2,3,...,N
                            s.t. sum(αi*yi) = 0
                                 0 <= αi <= C
    注意到上式中，低维特征仅仅以内积xi∙xj形式出现，如果我们定义一个从低维特征空间到高维特征空间的映射ϕ，将特征映射到高维度，让数据
 线性可分，我们就可以继续按照上面的目标函数来求解分类决策函数了，即：
                            min 1/2 * ( sum(αi*αj*yi*yj(ϕ(xi)∙ϕ(xj))) - sum(αi) ) ,其中i,j = 1,2,3,...,N
                            s.t. sum(αi*yi) = 0
                                 0 <= αi <= C
    可以看到，和线性可分的SVM的区别就是仅仅将内积 xi∙xj 替换为 ϕ(xi)∙ϕ(xj)
    看起来似乎是完美解决了线性不可分SVM的问题了，假如是2维特征的数据，可以映射到5维空间，如果是3维特征的数据，可以映射到19(3+6+10)维空间,
 随着低维特征不断增长，映射的高维特征则是爆炸性的增长，计算量太大，由此，引入核函数的概念!
   
   #假设ϕ是一个低维的输入空间χ到高维的希尔伯特空间的映射，那么存在函数K(x,z),对于任意x,z ∈ χ , 都有：
                                    K(x,z) = ϕ(xi)∙ϕ(xj)
    我们就称K(x,z)为核函数，仔细观察上式可以发现，K(x,z)的计算是在低维特征空间来计算的，它避免了在刚才我们提到了在高维维度空间计算内积的
 恐怖计算量。也就是说，我们可以好好享受在高维特征空间线性可分的福利，而又避免了高维特征空间恐怖的内积计算量。
 
 3.核函数的介绍
    事实上，核函数的研究非常的早，比SVM出现早得多，对于从低维到高维的映射，核函数不止一个，而我们一般说的核函数都是正定核函数，下面简单列举几个
 常见的核函数。
    (1)线性核函数
       linear kernel其实就是线性可分的SVM，表达式为：
                                    K(x,z) = x∙z
       即：线性可分SVM可以和线性不可分的SVM归为一类，区别仅仅在于线性可分SVM用的核函数为 linear kernel
    (2)多项式核函数
        poly kernel是线性不可分SVM常用的核函数之一，表达式为：
                                    K(x,z) = (γ*x∙z + r)^d
        其中，γ,r,d都需要自己调参定义
    (3)高斯核函数
       gaussian kernel，在SVM中也称为径向基核函数(RBF),它是非线性分类SVM最主流的核函数，libsvm默认的核函数就是它，表达式为：
                                    K(x,z) = np.exp(-γ*||x-z||^2)
       其中，γ大于0，需要自己调参定义
    (4)Sigmoid核函数
       Sigmoid kernel也是线性不可分SVM常用的核函数之一，表达式为：
                                    K(x,z) = tanh(γ*x∙z + r)       
       其中，γ,r需要自己调参定义
       
 小结：
    核函数的技巧就是把SVM的优化目标函数中的(xi∙xj)换成了K(xi,xj),在低维进行计算，而将实质上的分类结果表现在高维上，避免了在高维空间中的
 复杂计算。
"""
