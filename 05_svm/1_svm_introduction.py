# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/7/19 9:32 AM
 @desc: 本文在于引入SVM，介绍SVM的相关知识点，目的是在心中对SVM有一个大体的认识，本节不涉及代码的编写
"""

"""
 引子：
     SVM对于我而言，一直是一个黑匣子般的存在，为什么呢？函数间隔与几何间隔、间隔最大化的最优化问题、原问题与拉格朗日对偶问题、序列最小最优化
 算法以及核函数，以上都是在解释SVM算法中，都会出现的概念，好像它在对我说，“哦，这个你搞得懂，那接下来的这个你能搞得懂吗? 跪下吧，人类!”，
 抱着“你虽虐我千百遍，我仍待你如初恋”的态度，耐着性子，我尽力的将SVM算法挖掘到我所能到达的深度。
"""

"""
 1.SVM基本概念
    (1)一句话介绍SVM
         支持向量机，英文名Support Vector Machine，一般简称为SVM，通俗来讲，它是一个二分类模型，其基本模型定义为特征空间上的间隔最大的
      线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。
 
    (2)什么是线性分类器
         在上一章中，我讲解了logistic regression的算法原理，LR就是一个线性分类器，它是通过Logistic函数(或者sigmoid函数)，将自变量映射
      到(0,1)区间，以计算样本属于某一个类别的概率，最终得到的分类超平面为：w_0*1 + w_1*x_1 + w_2*x_2 = 0，在n维特征空间，分类超平面即为
      w.T * x + b = 0，在SVM中，分类决策函数为 y = sign(w.T * x + b), 其中sign(x)为符号函数。
      
    (3)函数间隔与几何间隔
       一般来说，一个点距离分类超平面的距离，可以表示分类预测的确信程度。假设某个点已正确分类，它离超平面越远，就认为它分类正确的确信度越高。
       在超平面 w.T * x + b = 0确定的情况下，|w.T * x + b|能够相对表示点x到超平面的距离，而(w.T * x + b) 与 y的符号是否一致，可以表示
    分类是否准确，因此为了表示分类的正确性以及确信度，引入了函数间隔的概念：
                                functional_margin = y(w.T*x+b)
       我们知道超平面的表达式为  w.T * x + b = 0，成比例的扩大w,b，超平面位置也不会改变，如2*w.T * x + 2*b = 0，但是函数间隔却变为了
    原来的两倍，因此，为了固定间隔的值，又引入了几何间隔的概念：
                                geometric_margin = y(w.T*x+b) / ||w|| , 其中||w||为L2范数
       可以得出函数间隔与几何间隔的关系为：
                                geometric_margin = functional_margin / ||w||
                                 
    (4)间隔最大化
       支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分类超平面。这个最大间隔分类超平面的问题，可以转换为下面的约束
    最优化问题：
                                max geometric_margin
                                s.t. yi(w.T*xi+b) / ||w|| >= geometric_margin ,其中i=1,2,3,...,N
                                
       用函数间隔代替几何间隔，约束最优化问题转化为：
                                max functional_margin / ||w||
                                s.t. yi(w.T*xi+b) >= functional_margin ,其中i=1,2,3,...,N
                                
    **而我们在前面知道，functional_margin与w,b是成比例变化的，它们所对应的分类超平面，仍然是同一个超平面，知道这一点之后，为了简化计算，
       
       我们不妨将上面的约束最优化问题中的functional_margin值定为1，w,b的取值自然也会成比例的变化，但是最终的分类超平面不会变，由此得到：
                                max 1 / ||w||
                                s.t. yi(w.T*xi+b) >= 1 ,其中i=1,2,3,...,N
                                
       而 max 1 / ||w|| 与 min 1/2 * ||w||^2 二者是等价的，进一步转化上述的约束最优化问题为：
                                min 1/2 * ||w||^2
                                s.t. 1 - yi(w.T*xi+b) <= 0 ,其中i=1,2,3,...,N
                         
       上面的这个表达式，数学基础比较好的人，一眼就能看出，这是一个凸二次规划问题，凸优化问题是指：
                                min f(w)
                                s.t. gi(w)<=0 , i = 1,2,3,...,k,共有k个不等式约束条件
                                     hj(w)=0, j = 1,2,3,...,l , 共有l个等式约束条件
                                     
       由此，我们终于完成了SVM算法的第一步，即定义算法的最优化问题，下面来讲解如何求解这个最优化问题。
                                     
    (5)拉格朗日对偶问题【重中之重】
       当我第一次看到这个名词的时候，第一反应是WTF，看到函数式时，我又立马关上书本，打出手机，播放歌曲，平复心情。。。
    废话还是不多说，其实耐心去看的话，实际上没有想象中的复杂，只是看起来繁琐而已。
          
       “在约束最优化问题中，常常利用拉格朗日对偶性将原始问题转换为对偶问题，通过解对偶问题而得到原始问题的解。”    -----《统计学习方法》
        看到这句话，我们心里肯定会想，什么形式的原始问题可以转换为对偶问题？什么是对偶问题？为什么解对偶问题可以得到原始问题的解？ 不着急，
    下面一一来做解释。
    
       (a)原始问题
          假设f(x),ci(x),hj(x)是定义在n维空间上的连续可微函数，考虑约束最优化问题：
                                min f(x) 
                                s.t. ci(x)<=0 , i = 1,2,3,...,k
                                     hj(x)=0 , j = 1,2,3,...,l   
          称此类约束问题为原始问题。
          乍一看，有没有觉得眼熟，没错，我们前面千辛万苦得到的SVM的约束最优化问题，就属于原始问题。
                                min 1/2 * ||w||^2
                                s.t. 1 - yi(w.T*xi+b) <= 0 ,其中i=1,2,3,...,N
                                     
       (b)极小极大问题
          首先引进广义拉格朗日函数，由于约束条件有k+l个，所以有：
          L(x,α,β) = f(x) + α0*c0(x)+α1*c1(x)+...+αk*ck(x) + β0*h0(x)+β0*h0(x)+...+βj*hj(x) ,不好写累加和符号，所以展开式表示
          
          于是，对x有限制条件的f(x)最优化问题，转变为了对x,α,β无限制条件的L(x,α,β)求极值问题，其中，αi,βj是拉格朗日乘子，αi>=0，
       再定义一个函数：
                                θp(x) = max L(x,α,β)  ,  计算在 α,β,αi>=0 条件下的最大值
          不妨来研究一下这个函数:
                                θp(x) = max (f(x) + α0*c0(x)+α1*c1(x)+...+αk*ck(x) + β0*h0(x)+β1*h1(x)+...+βj*hj(x))
                                
          假设某个x，不满足原始问题的约束条件，即有ci(x)>0或者hj(x)!=0，那么上式的结果为+∞，原因如下：
              1)假设c1(x)>0,而α1趋于+∞，其他各项αi,βj都取0时，最终结果为+∞;                         
              2)假设hj(x)!=0,而存在一个βj*hj(x)趋于+∞，其他各项αi,βj都取0时，最终结果还是为+∞;
          相反地，假设x满足约束条件，即ci(x)<=0且hj(x)=0时，而我们定义的αi>0，即每一项αi*ci(x)<=0，那么：    
              L(x,α,β)只能在所有的αk*ck(x)=0和βj*hj(x)同时取0时，才能得到最大值，这时候的最大值即为f(x).  【到这里一定要看懂】
          所以我们就得出来：
                                θp(x) = +∞   ，当x不满足约束条件时   
                                θp(x) = f(x) ，当x满足约束条件时
          根据这个式子，可以得出：
              min θp(x) = min max L(x,α,β) = min f(x)    其中,在min max L(x,α,β)中,min是对x求极小,max是对α,β求极大
          
          我们因此获得一个重要结论：即在x满足限制条件的前提下，min f(x) 等价于 min max L(x,α,β) ，这是一个极小极大问题
       
       (c)对偶问题
          将广义拉格朗日函数的极大极小问题表示为约束最优化问题：
                                max min L(x,α,β)     其中,min是对x求极小，max是对α,β求极大 
                                s.t. αi>=0,i=1,2,3,...,k
          称为原始问题的对偶问题
       
       (d)原始问题与对偶问题的关系
          若原始问题和对偶问题都有最优值，分别为d*,p*，则:
                    d* = max min L(x,α,β) <= min max L(x,α,β) = p*
          
          推论一：
             设 x*和(α*,β*)分别是原始问题和对偶问题的可行解，并且最优值d*=p*，则x*和α*,β*分别是原始问题和对偶问题的最优解。在某些条件下，
          原始问题和对偶问题的最优值相等，即d*=p*，这个时候，就可以用解对偶问题替代解原始问题。
          
          推论二：
             考虑原始问题和对偶问题，假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且假设不等式约束ci(x)是严格可行的，即存在x，对所有i
          有ci(x)<0，则存在x∗和α∗,β∗，使x∗是原始问题的解，α∗,β∗是对偶问题的解，并且有p∗=d∗=L(x∗,α∗,β∗).
          
          推论三：
             考虑原始问题和对偶问题，假设函数f(x)和ci(x)是凸函数，hj(x)是仿射函数，并且假设不等式约束ci(x)是严格可行的，则x∗和α∗,β∗分别
          是原始问题、对偶问题的解的充要条件是 x∗和α∗,β∗ 满足下面的KKT条件：
                                ∇x L(x∗,α∗,β∗) = 0
                                ∇α L(x∗,α∗,β∗) = 0
                                ∇β L(x∗,α∗,β∗) = 0                 (1)
                                αi*ci(x∗) = 0 , i = 1,2,3,...,k    (2)
                                ci(x∗) <= 0   , i = 1,2,3,...,k    (3)
                                αi* >= 0      , i = 1,2,3,...,k    (4)
                                hj(x∗) = 0    , j = 1,2,3,...,k    (5)
             (注：这里面所有的*号都不是乘号，由于下标看起来不够规范，本目录下附了KKT.png文件，看起来更直观一些)
             
             上面的KKT条件中，最重要的是(1)和(2)，(3)、(4)、(5) 都是将原始问题表示成拉格朗日函数时的条件.
             
          三个推论就是想告诉我们一句话：
             在满足KKT的条件下，原始问题可以转换为对偶问题进行求解，此时二者的最优值相等.
        
          这一部分的内容，主要是参考了《统计学习方法》P225 附录C 拉格朗日对偶性 
       
       (e)对偶问题的解法
          我们已经得出结论，原始问题的对偶问题，是一个极大极小问题，即 max min L(x,α,β) ,为了得到对偶问题的解,
          1)求出L(x,α,β)对x的极小值       ------ 做法：对x求偏导数,并令其等于0,解出x之后，把x的表达式再代入L(x,α,β)
          2)代入x值之后，再求出min L(x,α,β)对α,β的极大值
       这一部分的内容，详细请看《统计学习方法》P103 7.1.4 学习的对偶算法
       
       做了这么多功夫的意义何在呢？因为我们要把最初的原始问题转换到可以使用SMO算法来求解的问题，因为SMO算法是当下最流行的解法.我也不打算再
    往下挖了，讲道理，一直到原始问题转对偶问题的内容，我还大概能理解，后面对偶问题的求解，我就开始晕了，做了这么多的目的，是因为《机器学习实战》
    中，直接用的就是SMO算法来求解SVM，而我的目的也已经达到，就是原始问题是怎么转到对偶问题来的，至于二者的最优解在什么KKT条件下是相等的，或者
    求出来α*,β*之后，怎么求x*，我暂时也不想搞了，暂时先跑一下SMO算法再说吧.
        
       (补充：上面阐述了很多理论知识，听起来还是略显抽象，因此关于线性可分SVM的算法过程，我附上了网上的一篇总结，见svm_algorithm.png，
       另外，《统计学习方法》P107 例7.2是一个具体的案例题，做完之后能加深算法的理解)
                             
"""
