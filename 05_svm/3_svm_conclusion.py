# encoding: utf-8
"""
 @project:ML_Algorithms
 @author: Jiang Hui
 @language:Python 3.7.2 [GCC 7.3.0] :: Anaconda, Inc. on linux
 @time: 3/17/19 4:01 PM
 @desc:
"""
"""
 1.感受：
     SVM这个算法，是我目前整理机器学习笔记以来，耗费时间最长的算法，当然也是收获最多的算法，还是庆幸自己在整个过程中坚持下来了，中途无数次想过
 放弃的。现在来捋一下自己的学习所得吧：
     
                             SVM的分类超平面     w.T*x + b = 0
                                   ↓
                            函数间隔与几何间隔    geometric_margin = functional_margin / ||w|| 
                                   ↓
                               硬间隔最大化      max 1 / ||w||      s.t. yi(w.T*xi+b) >= 1 , i=1,2,3,..,N 
                                   ↓
                              最优化原始问题     min 1/2 * ||w||^2   s.t. 1 - yi(w.T*xi+b) <=0 , i=1,2,3,..,N 
                                   ↓
                                   ↓          (软间隔最大化与核函数，对偶问题的优化目标函数不变，只是α限制条件改变)
                                   ↓
                             拉格朗日对偶问题     p* = min max L(w,b,α) <= max min L(w,b,α) = q* , 满足KKT条件时等式成立
                                   ↓
                            SMO算法解对偶问题   (αi与αj的选择、αj_new_unc的计算与修剪、αi_new和b_new的计算、Ei的更新、迭代直至收敛)  
                                   ↓
                           由最优解α向量得出w和b
                               
 2.SVM算法的优缺点
    SVM算法是一个很优秀的算法，在集成学习和神经网络之类的算法没有表现出优越性能前，SVM基本占据了分类模型的统治地位。目前则是在大数据时代的大样
 本的背景下，SVM由于其在大样本时超级大的计算量，热度有所下降，但是仍然是一个常用的机器学习算法。
    
    优点：
        (1)解决高维特征的分类问题和回归问题很有效,在特征维度大于样本数时依然有很好的效果;
        (2)仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据;
        (3)有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题;
        (4)样本量不是海量数据的时候，分类准确率高，泛化能力强.
        
    缺点：
        (1)SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用;
        (2)非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数;
        (3)SVM对数据的缺失比较敏感.
 
 3.未完成的工作
    1)本次只实现了smo算法simple版本，full版本由于时间原因暂时没有实现，《机器学习实战》中有完整的代码；
    2)本次只讨论了SVM用于分类的整个思路，SVM其实也是可以用于回归的，但是这里也没有给出探讨.
          SVM回归可参考：https://blog.csdn.net/luoshixian099/article/details/51121767
                            
"""
